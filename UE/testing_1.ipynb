{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "import argparse\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import zipfile\n",
    "import os\n",
    "from Bio.PDB import PDBParser\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_structure(pdb_file):\n",
    "    # Parse the structure\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure('protein', pdb_file)\n",
    "\n",
    "    # Extract features from the structure\n",
    "    # This is a placeholder - you would need to replace this with real feature extraction code\n",
    "    features = np.array([len(structure)])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, zip_file, csv_file=None, labels_available=True, transform=None):\n",
    "        if labels_available:\n",
    "            self.labels_frame = pd.read_csv(csv_file)\n",
    "        self.zip_file = zipfile.ZipFile(zip_file, 'r')\n",
    "        self.transform = transform\n",
    "        self.labels_available = labels_available\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.labels_available:\n",
    "            return len(self.labels_frame)\n",
    "        else:\n",
    "            # Return the number of files in the zip file when no labels are available\n",
    "            return len(self.zip_file.namelist())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        pdb_id = self.labels_frame.iloc[idx, 0]\n",
    "        f = self.zip_file.open(f\"{pdb_id}_protein.pdb\")\n",
    "        pdb_data = f.read().decode()  # Read the PDB file into a string\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        # Parse the structure from the string\n",
    "        structure = parser.get_structure(pdb_id, StringIO(pdb_data))\n",
    "        # Preprocess your structure here\n",
    "        structure = preprocess_structure(structure)\n",
    "\n",
    "        if self.transform:\n",
    "            structure = self.transform(structure)\n",
    "\n",
    "        if self.labels_available:\n",
    "            labels = self.labels_frame.iloc[idx, 1]\n",
    "            labels = np.array([labels])\n",
    "            labels = labels.astype('float').reshape(-1, 1)\n",
    "            sample = {'structure': structure, 'labels': labels}\n",
    "        else:\n",
    "            sample = {'structure': structure}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProteinDataset(\n",
    "    'train.zip', csv_file='train.csv', labels_available=True)\n",
    "\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
    "valid_size = len(train_dataset) - train_size  # 20% for validation\n",
    "\n",
    "train_dataset, valid_dataset = random_split(\n",
    "    train_dataset, [train_size, valid_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(64*4*4*4, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Structure' object has no attribute 'readlines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\Bio\\File.py:72\u001b[0m, in \u001b[0;36mas_handle\u001b[1;34m(handleish, mode, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(handleish, mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m     73\u001b[0m         \u001b[39myield\u001b[39;00m fp\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Structure",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Move data to the correct device\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     structures \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mstructure\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "Cell \u001b[1;32mIn[28], line 27\u001b[0m, in \u001b[0;36mProteinDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m structure \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mget_structure(pdb_id, StringIO(pdb_data))\n\u001b[0;32m     26\u001b[0m \u001b[39m# Preprocess your structure here\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m structure \u001b[39m=\u001b[39m preprocess_structure(structure)\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     30\u001b[0m     structure \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(structure)\n",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m, in \u001b[0;36mpreprocess_structure\u001b[1;34m(pdb_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_structure\u001b[39m(pdb_file):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Parse the structure\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     parser \u001b[39m=\u001b[39m PDBParser(QUIET\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m     structure \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mget_structure(\u001b[39m'\u001b[39;49m\u001b[39mprotein\u001b[39;49m\u001b[39m'\u001b[39;49m, pdb_file)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# Extract features from the structure\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m# This is a placeholder - you would need to replace this with real feature extraction code\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mlen\u001b[39m(structure)])\n",
      "File \u001b[1;32md:\\Miniconda\\envs\\bioinfo\\lib\\site-packages\\Bio\\PDB\\PDBParser.py:97\u001b[0m, in \u001b[0;36mPDBParser.get_structure\u001b[1;34m(self, id, file)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstructure_builder\u001b[39m.\u001b[39minit_structure(\u001b[39mid\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[39mwith\u001b[39;00m as_handle(file) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m---> 97\u001b[0m     lines \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39;49mreadlines()\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lines:\n\u001b[0;32m     99\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEmpty file.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Structure' object has no attribute 'readlines'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move data to the correct device\n",
    "        structures = batch['structure'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(structures)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            # Move data to the correct device\n",
    "            structures = batch['structure'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(structures)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Valid Loss: {valid_loss/len(valid_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ProteinDataset('test.zip', labels_available=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move data to the correct device\n",
    "        structures = batch['structure'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(structures)\n",
    "\n",
    "        predictions.extend(outputs.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
